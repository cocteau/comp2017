:{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect All the Data\n",
    "------\n",
    "\n",
    "A lot of what we've done in this class revolves around data collection. We've learned how to hit the Twitter API, scrape web pages and fetch RSS feeds. Since data collection is such an important part of what you'll do in the upcoming sessions and beyond, let's get better at it. Like all skills, learning programming concepts take time, practice and repetition.\n",
    "\n",
    "Remember in the first few sessions we looked at Twitter Trends and tweets from the innaugration and #WomensMarch? Suman, Mark and I collected that data and turned it in to CSV for you to work with. Now that you've mastered the Twitter API, I'll show you how we collected and saved the data. We'll also look at collection data from a few other sources.\n",
    "\n",
    "In this notebook, we'll look at:\n",
    "- fetching Twitter Trends and storing them in CSV files\n",
    "- using the Twitter Streaming api to search by keyword as well as for tweets from a particular location\n",
    "- the difference between HTTP GET vs POST requests (remember HTTP??) and when we'd use one over the other\n",
    "- pulling information about the members of the Sentate and House using the Congress API from ProPublica\n",
    "- scraping Google News to get news about members from the Senate\n",
    "\n",
    "And, we'll end with a challenge! &#x1f3c6;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Trending API**\n",
    "\n",
    "Remember the Twitter Trending data we looked at in the first few classes? We collected the data via the Twitter API and convereted it in to CSV files for you. Since you are all pros at using the Twitter API, you can do the same.\n",
    "\n",
    "Start by getting our Twitter API initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab your keys from a previous notebook or https://apps.twitter.com\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# before we can make Twitter API calls, we need to initialize a few things...\n",
    "from tweepy import OAuthHandler, API\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter has a few Trends-related APIs** including one that allows you to see which locations have Trending data:\n",
    "\n",
    "https://dev.twitter.com/rest/reference/get/trends/available\n",
    "\n",
    "This API returns a list of locations (country, city, woeids) that you can then use to all the \"get trends for this location\" API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all of the trends locations\n",
    "# https://dev.twitter.com/rest/reference/get/trends/available\n",
    "\n",
    "avail_locations = api.trends_available()\n",
    "# avail_locations is a list so we can loop through and print them out\n",
    "for location in avail_locations:\n",
    "    print location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, wait!** Trying to decipher python dictionaries printed to the notebook can be painful. Let me show you a very simple trick to \"pretty print\" them. Use this method to print out dictionaries, lists, lists of dictionaries...pretty much anything! It makes it so much easier to look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pretty print to the rescue!\n",
    "import pprint\n",
    "\n",
    "def pretty_print(data):\n",
    "    # pass in a dictionary, list, etc and the ppprint module will print it out all nicely formatted\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's try this again\n",
    "for location in avail_locations:\n",
    "    pretty_print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now...let's get the current trends for New York. We use the `trends_place` API and pass the `woeid` (\"where on earth\" id):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get trends for New York\n",
    "woeid = 2459115\n",
    "ny_trends = api.trends_place(woeid)\n",
    "print type(ny_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise:** print out the trend and tweet volume for New York. Remember to use `pretty_print`, `type`, `len`, `keys` to help figure out the structure of the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's save the data to a CSV file** so we can do offline processing on it later.\n",
    "\n",
    "If we wanted to save the Trends data to a CSV file every 5 minutes, how would we do that? We can easily do that using the `csv` python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's save a few rows to a csv file\n",
    "import csv\n",
    "\n",
    "# open a file called \"our_data_file.csv\"\n",
    "# the 'w' means that we want to \"write\" to the file\n",
    "csv_file = open('our_data_file.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# write two rows to it\n",
    "# the important part here is that each row of the CSV file\n",
    "# is a python list. the csv module will convert your list\n",
    "# into the comma-separated line in the csv file\n",
    "csv_writer.writerow([1, 'myoung'])\n",
    "csv_writer.writerow([2, 'cocteau'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used Pandas to read CSV files, but you can also use the `csv` module. It doesn't give you the power of Pandas, but can be used to simply read in a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now, let's read the csv file\n",
    "\n",
    "# the 'r' means that we want to \"read\" to the file\n",
    "csv_file = open('our_data_file.csv', 'r')\n",
    "csv_reader = csv.reader(csv_file)\n",
    "\n",
    "for row in csv_reader:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now, let's save the New York twitter trends to a CSV file\n",
    "\n",
    "# open a file called \"ny_twitter_trends.csv\"\n",
    "# the 'w' means that we want to \"write\" to the file\n",
    "csv_file = open('ny_twitter_trends.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# new york woeid\n",
    "woeid = 2459115\n",
    "ny_trends = api.trends_place(woeid)\n",
    "\n",
    "for trend_info in ny_trends:\n",
    "    created_at = trend_info['created_at']\n",
    "    for trend in trend_info['trends']:\n",
    "        print trend['name']\n",
    "\n",
    "        row = [created_at, trend['name'], trend['tweet_volume']]\n",
    "\n",
    "        # write/save the row to the csv file\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what if we want to save the New York trends every 5 minutes?\n",
    "import time\n",
    "\n",
    "# open a file called \"ny_twitter_trends.csv\"\n",
    "# the 'w' means that we want to \"write\" to the file\n",
    "csv_file = open('ny_twitter_trends.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# new york woeid\n",
    "woeid = 2459115\n",
    "\n",
    "while True:\n",
    "\n",
    "    # get the trends for new york\n",
    "    ny_trends = api.trends_place(woeid)\n",
    "\n",
    "    for trend_info in ny_trends:\n",
    "        created_at = trend_info['created_at']\n",
    "        for trend in trend_info['trends']:\n",
    "            print trend['name']\n",
    "        \n",
    "            row = [created_at, trend['name'], trend['tweet_volume']]\n",
    "\n",
    "            # write/save the row to the csv file\n",
    "            csv_writer.writerow(row)\n",
    "            \n",
    "    time.sleep(60*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Twitter Streaming API **\n",
    "\n",
    "Twitter's *streaming API* \"filters\" tweets for citeria we are interested in, sending them through when there is a match. For example, we might look for tweets from a list of users, or for tweets that contain any word in a list of terms we provide or even tweets from users in a particular location. In short, with the streaming API, we see Twitter as a live data source.\n",
    "\n",
    "To get there, we need to create a \"listener\". This is done by making our own type of listener object. Yes, **our own object type.** We will say more about this later in the term, but the best way to make new types of objects in Python is to \"extend\" existing ones. Below, we take a Tweepy object type, a `StreamListener` and create a new kind of object called `MyListener`. We could call this type anything (`Bob` or `Emily` or `Lightbulb`) but it's always good to name things descriptively -- it makes your code more readble. \n",
    "\n",
    "Objects of type `StreamListener` have some base methods. They have various methods that describe actions they take in response to the Twitter feed. For example, our object type \"inherits\" a method called `on_status()` that is called when a tweet (as status update) matches our criteria. By default, it doesn't do much. So, for `my_listener`, we are going to start by simply printint out the text of the tweet. Easy! \n",
    "\n",
    "Here is the incatation to do this. For this notebook, we are going to simply make this method `on_status()` more elaborate. Again, all we are doing is taking the method that comes with the objects of type `StreamListener` from Tweepy and replace it with a method of our creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler, Stream, StreamListener\n",
    "\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    "    \n",
    "    def on_status(self, tweet):\n",
    "        # each time we get a new tweet, print it out\n",
    "        print tweet.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create an object of type `my_listener`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listener = MyListener()\n",
    "type(listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then create a call to Twitter's Stream API. Instead of using the function API as we did in previous notebooks, we are going to use the function Stream() -- it also takes our authenticatio keys and also a listener. We then `.filter()` the stream to look for references to `\"#MAGA\"`. We could put any number of terms in the list along with `#MAGA`.\n",
    "\n",
    "When you execute this cell, you will be tossed into a loop that is constantly filtering the stream of tweets, searching for our criteria. You stop it by interrupting the kernel (`Kernel -> Interrupt` using the buttons at the top of the Jupyter interface). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search for #MAGA\n",
    "stream = Stream(auth, listener)\n",
    "stream.filter(track=[\"#MAGA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about Trump?\n",
    "stream = Stream(auth,listener)\n",
    "stream.filter(track=[\"Trump\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to capture tweets from the streaming API and save them to a CSV file for later? Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    def initialize_csv_file(self, csv_file_name):\n",
    "        # open a file called \"streaming_tweets.csv\"\n",
    "        # NOTE: the 'a' means to append to the file\n",
    "        csv_file = open(csv_file_name, 'a')\n",
    "        self.csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    def on_status(self, tweet):\n",
    "        # save the user's screen name and tweet text to the csv file\n",
    "        self.csv_writer.writerow([tweet.user.screen_name.encode('utf-8'), tweet.text.encode('utf-8')])\n",
    "        \n",
    "        # let's also print out the tweet so we know it's working!\n",
    "        print tweet.text\n",
    "\n",
    "# create our listener object\n",
    "listener = MyListener()\n",
    "# open up the CSV file\n",
    "listener.initialize_csv_file('streaming_tweets.csv')\n",
    "# start the stream\n",
    "stream = Stream(auth, listener)\n",
    "stream.filter(track=[\"Sessions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's take a look at getting Tweets from a particular location**\n",
    "\n",
    "This is the API that I used to capture all of the Tweets from Washington DC during the innaugration and march.\n",
    "\n",
    "https://dev.twitter.com/streaming/overview/request-parameters#locations\n",
    "\n",
    "The input to the Twitter Streaming API is a \"bounding box\" - in other words, the latitude and longitude coordinates of an imaginary box drawn on a map. From Twitter: *the bounding box should be specified as a pair of longitude and latitude pairs, with the southwest corner of the bounding box coming first.*\n",
    "\n",
    "We can use this tool http://tools.geofabrik.de/calc/#type=geofabrik_standard&bbox=-74.186305,40.530653,-73.627396,40.969083&tab=1&proj=EPSG:4326&places=2 to help us create a bounding box for the New York Area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about by location?\n",
    "\n",
    "# create our listener object again\n",
    "listener = MyListener()\n",
    "# open up the CSV file\n",
    "listener.initialize_csv_file('streaming_new_york_tweets.csv')\n",
    "\n",
    "location_stream = Stream(auth, listener)\n",
    "location_bounding_box = [-74.19, 40.53, -73.62, 40.97]\n",
    "location_stream.filter(locations=location_bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detour on the architecture of web requests\n",
    "-----\n",
    "\n",
    "**CSV, PDF and HTML**\n",
    "\n",
    "So far, we have seen several ways to pull data from \"out there\" on the web. From the very first day, **we have been loading CSV files** into Pandas DataFrames using the function read_csv(). This function makes a request on our behalf to a web server for the CSV file we are after. It reads the file and converts is painlessly into a DataFrame. Easy. \n",
    "\n",
    "Then, last week, we expanded things a bit and started making requests for other kinds of files. **PDFs became a source of data.** We saw that we could have one of two kinds of PDF's -- those where the pages were stored as images and those that exposed the actual text. (We also saw a kind of hybrid -- a document that has been run through OCR by a scanner, producing a document that looks like an image, but the text is accessible, albeit with classic OCR mistakes, confusing \"1\" for \"I\".)  \n",
    "\n",
    "From there, we looked not to special files for data, but to **web pages** themselves. We recalled some of the basics of HTML and its rich tag language for describing documents. We saw how attributes added to tags to control their appearance (text size and color, font choice, and so on) via Cascading Style Sheets or CSS, also gave us clues to where the data we wanted might be hiding. We found attributes like \"class=\" or \"id=\" were used by designers to style pages and were descriptive enough to let us find data.\n",
    "\n",
    "We saw that PDFs and HTML documents, or web pages, are great at laying out text and making a page look \"just so.\" But they are rather poor formats for encoding data. Fishing around the source code of a `weather.gov` site to find the one paragraph tag &lt;p&gt; that contains the temperature for the Columbia campus. And we had to rip the text from a PDF, pulling a single long string from a page of a document, and pipe it through a regular expression get the list of companies Mnuchin will divest from. \n",
    "\n",
    "The data you find in a PDF or on a web page is meant to be read as you would read the page of a book. But in this class, we've seen that that kind of reading is labor-intensive. We want a computer to read for us instead -- to take in the data and create something new. This meant we wanted other formats, which would lead us to JSON and XML.\n",
    "\n",
    "**Web requests**\n",
    "\n",
    "Before we discuss these data formats, let's recall how we have been working with web pages. In some cases, we are interested in the page at a single web address, a URL or Uniform Resource Locator. The PDF's for Mnuchin, the Trump Dossier, and the Wikipedia page form Trump's cabinet are all examples.\n",
    "\n",
    ">https://en.wikipedia.org/wiki/Cabinet_of_Donald_Trump\n",
    "\n",
    "The page or file is fixed, and we inspect the content for the data we are after. Easy.\n",
    "In the case of the White House petitions, we saw that the URL itself was used to encode our data selection. We didn't want a single page, but a series of pages. Do we want the petitions on page 2? Page 3? The URL changes predictably with each different choice and we could write a program to pull them all. \n",
    "\n",
    "It is common to pass parameters to a web server this way, changing the URL. This is how the Twitter API works, for example. But we should make this mechanism clear. \n",
    "Before this class you were probably most familiar with simply entering information in a \"form\" and clicking \"submit\" to select the page you were after from a web server. For those of you with rusty HTML skills, a web form is just a special tag -- [here is a nice description of web forms.](http://www.w3schools.com/html/html_forms.asp) There are several different kinds of inputs we can be asked to supply, from clicking a \"radio\" button, to typing in simple text, to making a selection from a dropdown menu.\n",
    "\n",
    "There are two ways the information you enter into a form can be sent from your browser to the server -- They are called GET and POST and [they are well documented here](http://www.w3schools.com/tags/ref_httpmethods.asp). In short, **a GET method passes data (key-value pairs) in the URL being sent to the web site.** Take the White House petition site, for example. Or try a Google Search for \"Donald Trump Executive Orders.\" This request is also made via a GET request and generates generates the URL\n",
    "\n",
    "> https://www.google.com/search?q=Snap+IPO\n",
    "\n",
    "As I mentioned, there are a variety of inputs that are possible with a form and not just text, as in the case of a Google Search where you type your terms into a text box. You might select something from a drop-down menu or click a \"radio button\" to make some kind of a selection. \n",
    "\n",
    "All of these are coded in HTML as \"input\" tags, &lt;input&gt;, that appear in an overall \"form\" tag, &lt;form&gt;. The form for a Google search looks something like this -- albeit heavily edited to focus attention on the main parts.\n",
    "\n",
    "     <form action=\"/search\" method=\"GET\">\n",
    "        <input type=\"text\" maxlength=\"2048\" name=\"q\" value=\"\"> \n",
    "        <input type=\"submit\" value=\"Google Search\" type=\"submit\">\n",
    "        <input type=\"submit\" value=\"I'm Feeling Lucky\" type=\"submit\">\n",
    "    </form>\n",
    "The outer form and the input tags that are contained to specify how a user might alter their search. The second technique for sending data to a web server is known as POST. **POST encodes the data you want to send in the body of the request.** (Remember, we have added data to the header of an HTML request beforem appending our email addresses. This is similar.) GET exposes your request beause it is visible in the URL. POST is a little safer because the data is not so visible. It is not cached by your browser, nor does it appear in any web server logs. For these reasons, it's often used when you want to login to a web site or change your password, for example.\n",
    "\n",
    "You can tell what type you have, GET or POST, by looking at what happens when you submit your data and examine the URL  you're directed to. You can also look at the HTML of the page itself and see if the form has a method GET or POST. Twitter's login page, for example, features a POST request. \n",
    "\n",
    "       <form action=\"https://twitter.com/sessions\" method=\"post\">\n",
    "           <input type=\"text\" name=\"session[username_or_email]\" placeholder=\"Phone, email or username\"/>\n",
    "           <input type=\"password\" name=\"session[password]\" placeholder=\"Password\"/>\n",
    "           <input type=\"checkbox\" value=\"1\" name=\"remember_me\" checked=\"checked\"/>\n",
    "           <input type=\"submit\" value=\"Log in\"/> \n",
    "       </form>\n",
    "\n",
    "Again, the POST method specified here packages the name/value pairs inside the body of the HTTP request, which makes for a cleaner URL and imposes no size limitations on the form's output. It is also a little more secure.\n",
    "\n",
    "We bring this up because in some cases we will be able to reverse engineer the URL (as we did with the White House petitions) to get the data you need and in other cases you might have to look into the HTML. \n",
    "\n",
    "**A little in the weeds about URL encoding.**\n",
    "\n",
    "Because GET uses a URL to specify your data, we need to \"encode\" your data as a \"proper web address.\" Certain characters, like a \"space\" for example, cannot be part of a web address. The \"%20\" in our Google search is a \"URL encoded\" form of the space character. Why \"%20\"?\n",
    "\n",
    "Computers need to turn everything into 0's and 1's and that goes for characters as well. The system known as  [ASCII](https://en.wikipedia.org/wiki/ASCII) is one way to associate values of a numeric code (or binary digits known as bits) with letters and other symbols. The table below should make things clear. Start with the second column of symbols having \"Space\" at the top. See that each character is given a number and that number can be expressed in, say, binary (0's and 1's).\n",
    "\n",
    "<img src=https://cdn.sparkfun.com/assets/home_page_posts/2/1/2/1/ascii_table_black.png width=500>\n",
    "\n",
    "Notice that in ASCII the space character is number 32 or the binary string 100000. The URL encoding of a character is just the hexadecimal (base 16) representation of the ASCII number. So the space has a decimal value of 32 which is 20 in base 16 (2\\*16+0\\*1). You can read more about URL encoding [here](http://www.w3schools.com/tags/ref_urlencode.asp)\n",
    "\n",
    "**Post example**\n",
    "In some cases when looking at Congressional data sets, we need to know a representative's ID number. The [Biographical Directory of the United States Congress](http://bioguide.congress.gov/biosearch/biosearch.asp) is one way to look these up. \n",
    "\n",
    "Have a look at the page and fill in your favorite senator's name. Hit enter and see what happens. You see the URL at the top desccribes the service used to generate the page, but there is no hint of the data you provided. Go back to the [Biographical Directory of the United States Congress](http://bioguide.congress.gov/biosearch/biosearch.asp) page and look at it's source (in Chrome this means going to the \"View\" tab and selecting \"Developer\" and then \"Source\"). You will see something like (with extra lines between)\n",
    "\n",
    "> &lt;form method=\"POST\" action=\"http://bioguide.congress.gov/biosearch/biosearch1.asp\" &gt;<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;INPUT SIZE=30 NAME=\"lastname\" VALUE=\"\"&gt;<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;INPUT SIZE=30 NAME=\"firstname\" VALUE=\"\"&gt;<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...\n",
    "\n",
    "The \"action\" attribute gives you the location of the service we are after. And the various INPUT tags tell you what data you need to provide. In the cell below, we import from the \"requests\" module not get() but post(). We then fill in the input data as a dictionary. The rest of the request looks as it did for get(), except that our input_data is provided in the call to post(). Got it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here we do an HTTP POST request\n",
    "from requests import post\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The URL of the data service used by the Biographical Directory\n",
    "url = \"http://bioguide.congress.gov/biosearch/biosearch1.asp\"\n",
    "\n",
    "# The fields of the form on the page, represented as a dictionary\n",
    "input_data = {\"lastname\":\"schumer\",\n",
    "              \"firstname\":\"charles\",\n",
    "              \"position\":\"\",\n",
    "              \"state\":\"\",\n",
    "              \"party\":\"\",\n",
    "              \"congress\":\"\"}\n",
    "             \n",
    "# A header - this information is passed to the server to tell it \n",
    "# what kind of browser we're using (a bit of a fiction) and who we are\n",
    "head_data = {\"From\": \"markh@columbia.edu\"}\n",
    " \n",
    "# issue the POST request and collect the response as a JSON string\n",
    "response = post(url, data=input_data, headers=head_data)\n",
    "\n",
    "page = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print page.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Back to HTTP GET requests **\n",
    "\n",
    "Let's take a look at the [Congress API from ProPublica](https://propublica.github.io/congress-api-docs/). They require an API key like Twitter did, but in this case Derek Willis from ProPublica said we could all share mine (Mark's). This is the only time you can share mine. Ha! Look at what the API offers. There is information on members, on their voting patterns, on bills, on nominations. It's a rich set of data, all beaufifully organized.  \n",
    "\n",
    "Have a look at the documentation. It is an easy API. You submit special URLs and they return JSON data. The API key is sent in the header of the request, just like I used `From:` in previous notebooks (now we don't need `From:` because ProPublica knows who owns the key we are using -- me). To get a list of members in congress we use this URL.\n",
    "\n",
    ">`https://api.propublica.org/congress/v1/{congress}/{chamber}/members.json`\n",
    "\n",
    "You substitute in `{congress}` and `{chamber}` to represet which congress number (we are in the 115th session now) and whether we want the House or the Senate. Below we will take all current senators. Recall that the JSON is parsed with a call to `.json()` in the `response` object, just like the HTML was stored in `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "\n",
    "# define the url and put our keys in a dictionary -- propublica tells us\n",
    "# to call the header X-API-Key.\n",
    "url = \"https://api.propublica.org/congress/v1/115/senate/members.json\"\n",
    "head_data = {'X-API-Key':'90FfOvJfCWmOF3aPabqr1Fw5flx42Pm8GpjN4GT8'}\n",
    "\n",
    "# issue the request to their server for the url, incorporating the \n",
    "# header data\n",
    "response = get(url, headers=head_data)    \n",
    "\n",
    "# parse the json that's come back\n",
    "data = response.json()\n",
    "pretty_print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK this is a lot of JSON. The top is the important bit that we've copied here. We see that the result of our query to the API is a dictionary. We have keys like `\"copyright\"` and `\"results\"`. This latter key holds our Senate listing. It is a list of results, which in this case is just one element long. It is a dictionary that describes the `\"chamber\"` and the `\"congress\"` number and, as we wanted, the `\"members\"`. This last key holds a list, with one element for each member of the Senate. The data on each member is stored in dictionaries with the same entries. There are Facebook and Twitter accounts and so on. \n",
    "\n",
    ">`{u'copyright': u' Copyright (c) 2017 Pro Publica Inc. All Rights Reserved.',\n",
    " u'results': [{u'chamber': u'Senate',\n",
    "   u'congress': u'115',\n",
    "   u'members': [{u'api_uri': u'https://api.propublica.org/congress/v1/members/A000360.json',\n",
    "     u'domain': u'',\n",
    "     u'dw_nominate': u'',\n",
    "     u'facebook_account': u'senatorlamaralexander',\n",
    "     u'facebook_id': u'89927603836',\n",
    "     u'first_name': u'Lamar',\n",
    "    ...`\n",
    "\n",
    "So, if we give `response.json()` a name, say `data`, it looks like our members are in\n",
    "\n",
    ">`data[\"results\"][0][\"members\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Use `keys()`  on the dictionaries and  `len()` on the lists in `data` to verify that the information we're after is stored where we say it is. Once you've had a look through the payload, write some code below that prints out the Senator's name (first and last) and when they are up for reelection.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise #2: save the info (first, last, re-election date) to a new CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A bit about urlencoding ** while we scrape Google News for news about the senators.\n",
    "\n",
    "You can query google news for any term and receive an RSS feed of the results. \n",
    "\n",
    "Search for news about Snap: https://news.google.com/news?q=snap\n",
    "\n",
    "Now, get the results as RSS: https://news.google.com/news?q=snap&output=rss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get an RSS feed of news stories from Google news\n",
    "import urllib\n",
    "import feedparser\n",
    "\n",
    "# the url we want to request from Google will look something like this:\n",
    "# https://news.google.com/news?q=query-goes-here&output=rss&num=100\n",
    "\n",
    "name = 'Ron Wyden'\n",
    "params = {\n",
    "    'q': name,\n",
    "    'num': 100,\n",
    "    'output': 'rss'\n",
    "}\n",
    "\n",
    "# this is where the urlencode magic happens\n",
    "url_params = urllib.urlencode(params)\n",
    "\n",
    "# construct the final url\n",
    "url = 'https://news.google.com/news?' + url_params\n",
    "print url\n",
    "\n",
    "# make the HTTP request and parse the results\n",
    "feed = feedparser.parse(url)\n",
    "for entry in feed['entries']:\n",
    "    print entry['title']\n",
    "    print entry['link']\n",
    "    print '--'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1f3c6; **Challenge round!** &#x1f3c6;\n",
    "\n",
    "Pick one or two of these tasks and use your skills with web scraping to answer the question. In each case, there is a URL and a data question attached to it. These come mainly from an excellent list compiled by Dan Nguyen at Stanford.\n",
    "\n",
    ">Site: [https://analytics.usa.gov/](https://analytics.usa.gov/)<br>\n",
    "Task: Number of people visiting US Government web sites now<br><br>\n",
    "Site: [http://www.state.gov/r/pa/ode/socialmedia/](http://www.state.gov/r/pa/ode/socialmedia/)<br>\n",
    "Task: The number of Pinterest accounts maintained by U.S. State Department embassies and missions<br><br>\n",
    "Site: [http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx](http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx)<br>\n",
    "Task: In the most recently transcribed Supreme Court argument, the number of times laughter broke out<br><br>\n",
    "Site: [https://nfdc.faa.gov/xwiki/bin/view/NFDC/Construction+Notices](https://nfdc.faa.gov/xwiki/bin/view/NFDC/Construction+Notices)<br>\n",
    "Task: Number of airports with existing construction related activity<br><br>\n",
    "Site: [https://www.osha.gov/pls/imis/establishment.html](https://www.osha.gov/pls/imis/establishment.html)<br>\n",
    "The number of OSHA enforcement inspections involving Wal-Mart in California since 2014<br><br>\n",
    "Site: [https://www.tdcj.state.tx.us/death_row/dr_scheduled_executions.html](https://www.tdcj.state.tx.us/death_row/dr_scheduled_executions.html)<br>\n",
    "Task: Number of days until Texas's next scheduled execution <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&#x1f392; **Homework** &#x1f392;\n",
    "\n",
    "**First**, we want to create a proper data set around the Senators:\n",
    "1. information about each candidate\n",
    "2. news\n",
    "3. video clips\n",
    "4. and more!\n",
    "\n",
    "You already know how to do #1 and #2. Use the [YouTube Data API](https://developers.google.com/youtube/v3/getting-started) to get #3. What else could you include in this data set? Campaign contributions? Voting records? Pull down and store as much information about each candidate as you can.\n",
    "\n",
    "Other APIs that might be helpful:\n",
    "https://developers.google.com/youtube/v3/getting-started\n",
    "https://www.govtrack.us/developers/api\n",
    "https://propublica.github.io/campaign-finance-api-docs/\n",
    "\n",
    "\n",
    "**Second**, how will you interact with this data? Say I want to get the latest news and video clips on Ron Wyden or Kirsten Gillibrand - what's the best way to query and then view/read/watch that? Seems like all of the bot work you've been doing could be a good interface for interacting with this data set...\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
