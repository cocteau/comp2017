{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install seaborn\n",
    "pip install ggplot\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\"> Teaching Machines to Learn </h1>\n",
    "<hr>\n",
    "\n",
    "<img src=\"http://static1.businessinsider.com/image/535edec0ecad04c0741f732f/construction_google_car.gif\" style=\"width: 80%;\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A World of Signals: \n",
    "\n",
    "Perception, whether human or machine, is fundamentally dependent on our ability to *sense and analyze signals* that abound in the natural and digital world. Humans are evolutionarily powered to sense these, although variations in ability exists from person to person. But machines have to be taught first what perception means, and then taught how to keep learning on acquired perception principles. \n",
    "\n",
    "This task makes for some of the most fascinating and interesting challenges that exist in our quest to make machines smarter. It also automatically divides computing into many subfields, based on the signals that a machine will encounter and must analyze:  \n",
    "\n",
    "- When the signal is image/video/gif : *Computer Vision*\n",
    "- When the signal is text            : *Natural Language Processing*\n",
    "- When the signal is touch           : *Haptic Computing*\n",
    "- When the signal is sound           : *Speech Recognition / Audio Signal Processing*\n",
    "- When the signal is smell           : *There's [Cyranose!](https://en.wikipedia.org/wiki/Electronic_nose) / Classification of foods, bacteria detection [\\(you laugh\\)](http://www.disi.unige.it/person/MasulliF/papers/masulli-mcs02.pdf)*\n",
    "- When the signal is taste           : *we aren't there yet* \n",
    "\n",
    "The idea is that if machines can analyze these signals (themselves representations of the signals we as humans perceive) accurately, they will have intelligence in dealing with situations that humans have to deal with on a daily basis. For example, a machine could analyze an image and recognize smiling faces. It can analyze text and recognize abusive language. It can analyze speech tones and recognizes distress or strain. It can sense the pressure of a touch, and that magnitude of pressure indicates different intentions of the user. As you can see, all of these so called \"learnings\" are trying to make machines perceive and analyze signals like humans do. \n",
    "\n",
    "The one caveat is: Machines can do it a million times faster than a human.\n",
    "\n",
    "Apart from perception and processing, machines can also **LEARN** to perform activities that humans do ...tasks varying from driving a vehicle to something much esoteric.. such as copying artistic styles. \n",
    "\n",
    "<img src = \"https://i.imgur.com/sb8dHcY.png\" width=60%></img>\n",
    "<br>\n",
    "\n",
    "Machine Prediction is every bit as awesome, magical and fearsome as you can imagine. At the core, what a human brain does is match patterns and then predicts. And everything you feel is based on how well that prediction is going. For example, dopamine - the molecule behind our most fundamental cravings - [is a prediction error system](https://medium.com/the-spike/the-crimes-against-dopamine-b82b082d5f3d). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AI and Machine Learning\n",
    "\n",
    "Originally there were three subdivisions of AI: (1) Neural Networks, (2) Genetic Programming (Evolutionary Computing) and (3) Fuzzy Systems. As data became abundantly available and computation became cheaper and more powerful, a more statistical approach came into the forefront. This was when machine learning was born. You will see the terms 'AI' and 'Machine Learning' interchanged often. Machine Learning is a *type* of AI that is heavily dependent on data.\n",
    "\n",
    "- Machine Learning: The ability of computers to learn from data without an **explicitely** pre-programmed rule set.\n",
    "- Artificial Intelligence: Intelligence exhibited by machines. \n",
    "\n",
    "\n",
    "\n",
    "| Method | Learning Model | Improvement Criteria | ~Year | Pitfalls | \n",
    "| ------ | ----------- | ----------- | ----------- | ----------- | \n",
    "|1. Old AI   |  Precoded Rules | X | 1950s | Too few rules |\n",
    "|2. Expert Systems | Inferred Rules | X | 1970s | Knowledge Acquisition problem |\n",
    "|3. AI Winter | :( | :( | 1980s |  :( |\n",
    "|4. Machine Learning | Data | Experience + Reinforcement| 1990s | Black box models |\n",
    "|5. Deep Learning | Lots of Data | Experience + Reinforcement + Memory | 2000s | Cannot explain itself (yet) |\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = \"https://qph.ec.quoracdn.net/main-qimg-d49da0fd1ac86b19d4e67d153926c026-p\" width= 70%></img>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Your ML ability is limited by the data you have\n",
    "\n",
    "There are many things you have to do before you get to the \"algorithm\" or \"modeling\" phase of a machine learning system. The chief among these is to transform/ format the data so it is easily ingestable by the algorithm. You must also look if your data is biased (it will be but you have to be aware of it). Then you must choose a type of machine learning algorithm to use. This is almost always dependent on the kind of data you have (more below). You can always run multiple algorithms on your data set and test the outcome of which model performs better. \n",
    "\n",
    "<br>\n",
    "Samples? Lables? Categories? What are these...\n",
    "\n",
    "<html>\n",
    "<img src = \"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2015/04/drop_shadows_background2.png\" width = 90%>\n",
    "\n",
    "</html>\n",
    "<br>\n",
    "\n",
    "As you can probably guess from the figure, the three big factors in what model gives the best performance is mainly dependent on: \n",
    "- (1) how many samples of data do you have, \n",
    "- (2) do you have labels for your data instances and \n",
    "- (3) is your label categorical?\n",
    "\n",
    "So lets quickly list the topics that we have/will covered:\n",
    "\n",
    "\n",
    "> Data Ingestion\n",
    "    - Data Formats (e.g., dataframes, dictionaries) **\n",
    "    - Data Discovery **\n",
    "    - Data Acquisition **\n",
    "    - Integration and Fusion   (beware of Simpson's Paradox)\n",
    "    - Transformation + Enrichment **\n",
    "    \n",
    "\n",
    "> Data Munging\n",
    "    - Principal Component Analysis (PCA)  ** \n",
    "    - Dimensionality Reduction **\n",
    "    - Sampling\n",
    "    - Denoise\n",
    "    - Feature Extraction\n",
    "    \n",
    "> Types\n",
    "    - Supervised (I know the label of each data instance. )\n",
    "    - Unsupervised (I do not know the label of any data instance. ) \n",
    "    - SemiSupervised ( some labeled, mostly unlabeled) \n",
    "    \n",
    "> Supervised Algorithms:  \n",
    "    - Decision Trees (this class)\n",
    "    - Random Forests (this class)\n",
    "    - Linear Regression (this class)\n",
    "    - Support Vector Machines (next class)\n",
    "    \n",
    "> Unsupervised Algorithms: \n",
    "    - Kmeans ** \n",
    "    - Neural Nets (next class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Here's what a ML pipeline looks like.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "<html>\n",
    "<img src= 'http://sumandebroy.com/columbia/images/mlloop.png' width=100%>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here are the basic steps in building machine learning algorithm:*\n",
    "1. Signal Detection: find a source, check if it generates a signal\n",
    "2. Estimation: Give values to those signals. E.g, each like button press = +1 POS vote but each heart/star press = +2 POS votes. \n",
    "3. Sample which parts of the data you want to use, or is usabe.\n",
    "4. Split your data into 60%-40% between training & test. Training data is used to BUILD the model. Test data is used to EVALUATE the model. Ideally, you'd also keep some for validation, which is used to TUNE the model. \n",
    "5. Have a reinforcement framework so your model can improve over time.\n",
    "\n",
    "*Things to think about*:\n",
    "- Which nodes are most manual ?\n",
    "- In which nodes can bias creep in.. and how?\n",
    "- Which nodes lead to black box?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A tale of terminology**: Machine learning uses statistics extensively in every conceivable way. Yet, you will find people using two different terminologies sometimes.\n",
    "\n",
    "- Statistical Learning : Infer the process by which data you have was generated (Inference)\n",
    "- Machine Learning: Know how you can predict what future data will look like w.r.t. some variable (Prediction)\n",
    "\n",
    "Now this can start many flame wars, some people call machine learning \"glorified statistics\". But in such discussions always remember Ken Thompson's quote:\n",
    "> *when in doubt, use brute force.*\n",
    "\n",
    "Real world data can be messy, with incredibly complex feedback loops. When the assumptions are hard to catch or it is safer than guessing them wrong, *prediction is a more robust bet than inference*. This is why so many have embraced ML, because its safer to build software that predicts and then tune, rather than make assumptions about the data generating source. This is especially true if your training data is not large enough compared to the no. of features. \n",
    "\n",
    "Machine Learning makes no prior assumptions about the underlying relationships between the variables. You just throw in all the data you have, and the algorithm processes the data, discovers patterns - using which you can make predictions on the new data set. But this has its own pitfalls.. no free lunch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. So what is a model\n",
    "\n",
    "Obviously the model is a result of supervised or unsupervised learning methods applied to data. \n",
    "\n",
    "Labeled Data can be priceless: Its hard to get and difficult to implement. Most people use tools like [Mechanical Turk](http://neerajkumar.org/writings/mturk/). If labeled data is the future, all of the jobs that are taken away from AI might be replaced by labelling jobs. That'd make an interesting distopia. Ironically, that means we automated ourselves back into manual labor. \n",
    "\n",
    "Anyways, we will start with the simplest of models... \n",
    "<br>\n",
    "\n",
    "### 5.1 Linear Regression: \n",
    "\n",
    "A linear regression takes a bunch of data, and attempts to find the relationship between the independent variable (\"cause\", X) and a dependent variables (\"results\", Y). To start, given a data set with two columns, X and Y, its task is to find a line that best describes Y as a function of X. It is used to figure out serious things in the real world like GDP, exchange rates, money flows, etc. and is a heavily used research tool in the social and political sciences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some simple data y = 2*x + 4+ error\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "data = DataFrame({\"x\":np.random.randn(20)})\n",
    "data[\"y\"] = 3*data[\"x\"]+4+2*np.random.randn(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we are going to plot this using plotly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotly.plotly import iplot, sign_in\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "sign_in(\"cocteautt\",\"9psj3t57ti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace0 = Scatter(x=data['x'],y=data['y'],mode=\"markers\",name=\"data\")\n",
    "mydata = [trace0]\n",
    "iplot(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a model that tries to fit this data. we start with linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(data[[\"x\"]], data[\"y\"])\n",
    "\n",
    "print model.intercept_\n",
    "print model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace0 = Scatter(x=data['x'],y=data['y'],mode=\"markers\",name=\"data\")\n",
    "trace1 = Scatter(x=data['x'],y=model.predict(data[['x']]),name=\"regression line\")\n",
    "mydata = [trace0,trace1]\n",
    "iplot(mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sidebar: The Grammar of Graphics**\n",
    "\n",
    "Let's look at some real data, this time using the \"hardest\" data set we've seen before. To help us with the data visualization, we are going to use ggplot (and not rely on plotly any longer). We are going to import all of the \"names\" in the ggplot package. In previous import statements we have selected a single name or a comma separated list of names. Here we take all the names that the package knows about -- all the functions, the variables, the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "hardest = read_csv(\"hardest_small.csv\")\n",
    "hardest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At their most basic, ggplots take 2 arguments: a **data frame** and accompanying **\"aesthetics\"** or aes object. Aesthetics define how ggplot will extract data from your data frame and render it. Think of it as the instructions for creating x, y, color, etc. components.\n",
    "\n",
    "An aes object is just a dictionary with keys being an aesthetic property and values being strings or formulas relating to data in your data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aes(x='education', y='income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our first ggplot. It's really just onto which we will place data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x =\"education\",y='income'),data=hardest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will (quite literally) add a scatterplot (geom_point) to our canvas. We'll get into more detail on how this works later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x =\"education\",y='income'),data=hardest)+geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this a little clearer, the grammar breaks the components of a graphic down into various pieces.\n",
    "\n",
    "* **data** in ggplot, data must be stored as a pandas data frame\n",
    "* **a coordinate system** describes 2-D space that data is projected onto (for example, Cartesian       coordinates, polar coordinates, map projections, and so on)\n",
    "* **geoms** describe type of geometric objects that represent data (for example, points, lines,   \n",
    "  polygons)\n",
    "* **aesthetics** describe visual characteristics that represent data (for example, position, size,   color, shape, transparency, fill)\n",
    "* **scales** for each aesthetic, describe how visual characteristic is converted to display values   (for example, log scales, color scales, size scales, shape scales, ...\n",
    "* **stats** describe statistical transformations that typically summarize data (for example,      \n",
    "  counts, means, medians, regression lines)\n",
    "* **facets** describe how data is split into subsets and displayed as multiple small graphs\n",
    "\n",
    "geom_point() says that we want to render our x and y data as points. We can further adapt them by assigning colors and other features. \n",
    "\n",
    "Make another scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, scatterplots are hard to read on their own, perhaps because of overplotting. We can introduce a \"trend line\" by adding statistical artifacts to the plot. Here we use a \"smoother\" -- Think of Galton dividing his data into bins on the x-axis, finding the mean of the y-values in each bin, plotting them and then connecting the dots. That's essentially what's going on here plus or minus some bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x='education',y='income'),data=hardest)+geom_point()+stat_smooth(method=\"lm\",color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can add a lot of other components to a plot (again, literally adding them)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x='education',y='income'),data=hardest)+\\\n",
    "    geom_point()+stat_smooth(method=\"lm\",color=\"blue\")+\\\n",
    "    ggtitle(\"Life expectancy and obesity rates\")+\\\n",
    "    xlab(\"Percentage with College Education\")+\\\n",
    "    ylab(\"Median income in the county\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x='education',y='unemployment'),data=hardest)+\\\n",
    "    geom_point()+stat_smooth(method=\"loess\",color=\"blue\")+\\\n",
    "    ggtitle(\"Unemployment and education rates\")+\\\n",
    "    xlab(\"Percentage with College Education\")+\\\n",
    "    ylab(\"Unemployment rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn # not necessary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Decision Trees\n",
    "\n",
    "Decision trees are predictive models that maps features of items (represented by nodes) to their target labels (represented by leaves of the tree). Thus when a data instance encounters a decision tree model, it must traverse through the nodes to be labeled by one of the leaves. The nodes it chooses are based on the features the data instance posesses. \n",
    "<br>\n",
    "\n",
    "<html>\n",
    "<img src=\"http://sumandebroy.com/columbia/images/dtree.gif\" ></img>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Making a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some basic code to visualize a decision tree boundaries. \n",
    "def visualize_tree(estimator, X, y, boundaries=True,xlim=None, ylim=None):\n",
    "    estimator.fit(X, y)\n",
    "\n",
    "    if xlim is None:\n",
    "        xlim = (X[:, 0].min() - 0.1, X[:, 0].max() + 0.1)\n",
    "    if ylim is None:\n",
    "        ylim = (X[:, 1].min() - 0.1, X[:, 1].max() + 0.1)\n",
    "\n",
    "    x_min, x_max = xlim\n",
    "    y_min, y_max = ylim\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, alpha=0.2, cmap='rainbow')\n",
    "    plt.clim(y.min(), y.max())\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)        \n",
    "    plt.clim(y.min(), y.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from ipywidgets import interact\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DEPTH = 2\n",
    "clf = DecisionTreeClassifier(max_depth=DEPTH, random_state=0) # \n",
    "result = visualize_tree(clf, X, y)\n",
    "interact(result, depth=[1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now try with DEPTH =3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** But the classification quality can vary in every run: ** For example, take a look at two trees built on two subsets of this dataset. The details of the classification are very different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[:200], y[:200], boundaries=False)\n",
    "plt.figure()\n",
    "visualize_tree(clf, X[-200:], y[-200:], boundaries=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A common way to address such deviances is to use an [Ensemble Method](http://scikit-learn.org/stable/modules/ensemble.html#forest): this is a meta-estimator which essentially averages the results of many individual estimators which over-fit the data. The most common ensemble method is a Random Forest, in which the ensemble is made up of many decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Enter Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_randomized_tree(random_state=0):\n",
    "    X, y = make_blobs(n_samples=300, centers=4,\n",
    "                      random_state=0, cluster_std=2.0)\n",
    "    clf = DecisionTreeClassifier(max_depth=15)\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    i = np.arange(len(y))\n",
    "    rng.shuffle(i)\n",
    "    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n",
    "                   xlim=(X[:, 0].min(), X[:, 0].max()),\n",
    "                   ylim=(X[:, 1].min(), X[:, 1].max()))\n",
    "    \n",
    "from IPython.html.widgets import interact\n",
    "interact(fit_randomized_tree, random_state=[0, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the details of the model change as a function of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "visualize_tree(clf, X, y, boundaries=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When is a random forest not **that** useful:\n",
    "1. Stuctured data like images where a Neurel Net might do better\n",
    "2. Small data.. might lead to overfitting\n",
    "3. High dimensional data (sometimes)\n",
    "    \n",
    "But in general decision trees and random forests are very robust models, and you can do very interesting game AIs with it.\n",
    "<br>\n",
    "<img src = \"https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/11fa695a4e6b0f20a396edc4010d4990c8d29fe9/0-Figure1-1.png\" width = 70%> </img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Considerations\n",
    "\n",
    "### 6.1 Scalability:\n",
    "\n",
    "Sometimes **a less complex model can be more scalable**. Most modern prediction systems is a series of tradeoffs. Scalability of a model pertains to the fact that if the distribution (or pattern) of the input changes, how easily can the model adapt to it. Complex models are powerful but you need to make sure the distribution of your input will remain the same. \n",
    "\n",
    "<img src=\"http://metamarkets.com/wp-content/uploads/2011/03/photo-1-1024x768.jpg\" width = 50%> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Bias and Variance  - The model moves with the data\n",
    "\n",
    "Any learning algorithm has errors that come from two sources..BIAS and VARIANCE. \n",
    "\n",
    "Bias is the tendency of your algorithm to consistenly not take all information into account, thus learning the wrong thing. This leads to UNDERFITTING. Variance is your algorithm's tendency to learn random things irrespective of the real signal. This leads to OVERFITTING. So the final thing we need to note is that models overfit and underfit. Here's how to intuitively understand this. \n",
    "<br> \n",
    "\n",
    "<img src = \"https://qph.ec.quoracdn.net/main-qimg-f9c226fe76f482855b6d46b86c76779a-p\" width=50%></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A person with high bias is someone who starts to answer before you can even finish asking. A person with high variance is someone who can think of all sorts of crazy answers. Combining these gives you different personalities:\n",
    "\n",
    "- High bias/low variance: this ismsomeone who usually gives you the same answer, no matter what you ask, and is usually wrong about it;\n",
    "\n",
    "- High bias/high variance: someone who takes wild guesses, all of which are sort of wrong;\n",
    "\n",
    "- Low bias/high variance: a person who listens to you and tries to answer the best they can, but that daydreams a lot and may say something totally crazy;\n",
    "\n",
    "- Low bias/low variance: a person who listens to you very carefully and gives you good answers pretty much all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next class:\n",
    "\n",
    "1. *Predicting Titatic survivors*: One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. Our task will be to predict who had a greater chance of survival. \n",
    "\n",
    "2. *Neural Networks* : Attempting to model the brain\n",
    "\n",
    "3. *Data-Driven Bugs* : How they are born and what they do\n",
    "\n",
    "4. *Debugging and Explainable AI* : The more you learn about machine learning, the more you realise that debugging tools & a clear understanding of how the algorithms you’re using work are totally essential for making your models better. \n",
    "\n",
    "<br>\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "f2d6749402b64b7fae177c74d947ea42": {
     "views": [
      {
       "cell_index": 44
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
