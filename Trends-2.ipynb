{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trends in Twitter\n",
    "----------------\n",
    "\n",
    "Last time we looked extensively at a dataset consisting of Facebook's trending topics for 5 zones. Today, we are going to dig in to Twitter. I have cleaned up the original Twitter data a fair bit. Recall that the ingestion script probes Twitter every 15 minutes and logs the TTL (trending topic list) + underlying trends (users might not see this in the UI) for ~400 geographical locations worldwide. Again, [download the data](http://compute-cuj.org/twitter_trending_topics_for_us_120to122_mh2.csv.gz), uncompressit, and place it in the same folder as this notebook.\n",
    "\n",
    "The data clean up some issues with the timing of the robots and makes life generally happier. So, load it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv, set_option\n",
    "set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends = read_csv('twitter_trending_topics_for_us_120to122_mh2.csv')\n",
    "trends.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trends.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are sorted first by city and then by time, running from oldest to newest topic. The display of the first 50 lines shows this structure clearly. After 29 trends from 8 minutes after midnight January 20, we jump to the trends from Albuquerque 23 minutes after midnight. As with FB, the data collection here is meant to be every 15 minutes. \n",
    "\n",
    "We can look at the first and the last entries to get a sense of the period of our data collection from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, data were collected from twitter from midnight January 20th until just before midnight on January 22nd. As with Facebook, each pass of data collection scooped up all the trends for all the locations in the US. We see that there are 64 unique places (cities + the United States) and we can look at how many trends we have from each..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends[\"city\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends[\"city\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 3 days of data collection which is 3\\*24\\*4 wich is about 300 observation times. If we have about 11,000 topics for each city, that's about 35 or so trends per city for every time we pulled data from Twitter. \n",
    "\n",
    "**Las Vegas (and Pandas Series)**\n",
    "\n",
    "Now, let's pick a city and look at its trends. Las Vegas is a reasonably large city. Let's look at what kinds of topics trended there. We would usually use value_counts() to summarize a qualitative data column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trendy = trends[ trends[\"city\"] == \"Las Vegas\" ]\n",
    "topics = trendy[\"topic_name\"].value_counts()\n",
    "\n",
    "type(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value_counts() method returns a single Pandas column of data, an object of type \"Series.\" You can think of a Series as a list, but one that might have names attached to each entry. So our value_counts() Series has its core \"list\" data (the number of times each topic appeared in Las Vegas, say) but each data point also has a label (the topic name). \n",
    "\n",
    "So **you can subset it just like you would a list**. Here we take the first 25 most frequent topics that crested the top 10. Remember how slices work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the names. Pandas gives you the ability to use not just row numbers (as we have been doing) but also row names (strings that describe each row rather than a number). It calls them an \"index\". So far, our index has always been just a row number. Here's our trends data, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get access to the index of a DataFrame or a series by referring to \".index\". It returns something that is again like a list. Here is the index for our \"topics\" Series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, becaue the index is list-like, you can make a subset. Here we look at just the first 25 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tops = topics.index[:30]\n",
    "tops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then ask if various strings are in this (essentially) list. The operator \"in\" does that for for single strings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"#ThankYouObama\" in tops\n",
    "print \"Columbia Journalism\" in tops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and if our strings are in a Pandas DataFrame, we can use the equivalent operator \"isin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trendy[\"topic_name\"].isin(tops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"isin\" operator makes us a mask that we can use to subset our Las Vegas trends data, keeping only the rows associated with the top 25 trends, say. We can then plot these top 25 in a dot chart. I've make the code for plotly a bit easier to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotly.plotly import iplot, sign_in\n",
    "import plotly.graph_objs as go \n",
    "\n",
    "sign_in(\"cocteautt\",\"9psj3t57ti\")\n",
    "\n",
    "trendy_tops = trendy[trendy[\"topic_name\"].isin(tops)]\n",
    "\n",
    "mydata = [go.Scatter(x=trendy_tops[\"datetime\"],y=trendy_tops[\"topic_name\"],mode=\"markers\")]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=800,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods (some generality for examining trends)**\n",
    "\n",
    "Methods are a list of features that trend signals usually contain. While these might not be valuable individually to comprehend the nature of the trend, a combination of all features allows two things:\n",
    "\n",
    "* Uniquely fingerprint a trend in the world\n",
    "* Allow comparison between two trend signals.\n",
    "\n",
    "\n",
    "**Origins (which has to do with time) and geospan**\n",
    "\n",
    "The origin of a trend indicates where it was initiated. The main reason origin is a fascinating features includes (1) big trends can originate in small cities and go national (2) sometimes cities have an affinity to initiate certain categories of trends, e.g., many gaming trending topics originate in SF whereas numerous fashion trending topics originate in NY. \n",
    "\n",
    "The trend \"To Sir With Love\" is presumaby from the SNL skit in which two cast members said goodbye to President Obama. The times here require a little work. The datetime column ends in \"Z\" which means the time is recorded in UTC, Coordinated Universal Time. It is 5 hours ahead of NYC now. So this means the trend started before 1am NYC time.\n",
    "\n",
    "Let's see where else it trended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_topic = 'To Sir With Love'\n",
    "\n",
    "trends_topic = trends[trends['topic_name']==target_topic].sort_values(by=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata = [go.Scatter(x=trends_topic[\"datetime\"],y=trends_topic[\"city\"],mode=\"markers\")]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=800,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends_topic.sort_values(by=\"timestamp\").head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn**\n",
    "\n",
    "We began by looking at trends in Las Vegas and then moved out to various cities. We can extend this analysis in various ways. Starting with another city or the United States? Looking for less common trending topics (not the top 25 but maybe a middle 25? What else?\n",
    "\n",
    "Try!\n",
    "\n",
    "I've pulled the important code into four short steps. There's a lot of, um, pedagogy up there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. load up pandas and then read in the data\n",
    "from pandas import read_csv,set_option\n",
    "\n",
    "trends = read_csv('twitter_trending_topics_for_us_120to122_mh2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. look at one city\n",
    "trendy = trends[ trends[\"city\"] == \"Las Vegas\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. pull the top trending topics (maybe <= 10 is too much? is the top 25 right?)\n",
    "topics = trendy[\"topic_name\"].value_counts()\n",
    "tops = topics.index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. prepare to plot trends from the city\n",
    "from plotly.plotly import iplot, sign_in\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "sign_in(\"cocteautt\",\"9psj3t57ti\")\n",
    "\n",
    "trendy_tops = trendy[trendy[\"topic_name\"].isin(tops)]\n",
    "\n",
    "mydata = [go.Scatter(x=trendy_tops[\"datetime\"],y=trendy_tops[\"topic_name\"],mode=\"markers\")]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=800,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Look at a single trend and plot it\n",
    "target_topic = 'To Sir With Love'\n",
    "\n",
    "trends_topic = trends[(trends['topic_name']==target_topic)]\n",
    "\n",
    "mydata = [go.Scatter(x=trends_topic[\"datetime\"],y=trends_topic[\"city\"],mode=\"markers\")]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=800,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **geospan** of a trend signal signifies the various geo-locations at which it was observed. In the case of micro signals, this boils down to individuals from different locations acting upon the media related to the trend. Geospan's are an important measure in identifying if a trend has gone national, in which case it will be visible in most geo locations of the country!\n",
    "\n",
    "**Persistence**\n",
    "\n",
    "Persistence of a trend is the duration of continuous time units for which it kept trending in some geo-location, signified by continual presence in the trending topic list. This means during the persistence spell, a trend never fell out of the TTL and was not replaced by any other trend. \n",
    "\n",
    "So what does persistence really signify? Recall that a topic trends because people are tweeting about it. Two conditions are necessary for a trend to persist: \n",
    "1. a decent volume of tweets containing the trending word in a short amount of time and \n",
    "2. a failure of consolidation - i.e.  other tweets from the user group (either geo-location or follower group) fail to use the same trending word/ hash-tag in a consolidated fashion in enough tweets. This is also [the reason why #OccupyWallStreet **did not** trend in New York](http://www.niemanlab.org/2011/10/why-hasnt-occupywallstreet-trended-in-new-york/). \n",
    "\n",
    "\n",
    "<img src = \"http://www.niemanlab.org/images/socialflow_twittertrending.png\">\n",
    "\n",
    "The first condition assures that the word is trending enough to be above the threshold or cut-off marker that qualifies as a trend. The second condition assures that other trends are not competing hard enough to enter into the TTL. \n",
    "\n",
    "A smart way scientist visualize persistence is through something called dispersion plot. The Y-axis represents geo locations whereas the X-axis represents units of time since origin. You can the place a (dot) for every time the trend was observed at a location, and a blank if it wasn't. The result is continuos lines indicating persistence and gaps indicating lack of it. \n",
    "\n",
    "As we said, the persistence of a trend can be defined as the longest sequence of consecutive time periods that it was popular. We might take that to mean it was in the top 10, say. The sequence of consecutive time periods can be turned into actual time. If a topic persisted for 20 time periods that's 20\\*0.25 = 5 hours. \n",
    "\n",
    "Let's pick a trend and a city and see how what it's persistence is like. We start by creating a smaller DataFrame that has just one city's trends. We then add a new column called \"tops\" to this DataFrame that is True if the position is 10 or smaller and False otherwise. We add the 0 to the Boolean because Python will convert a True to 1 and a False to 0 when you include it in an arithmetic calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_trend = '#USofScience'\n",
    "city = 'Seattle'\n",
    "\n",
    "trendy = trends[(trends['topic_name'] == target_trend) & (trends['city']==city)].sort_values(by='timestamp')\n",
    "\n",
    "# add a new column\n",
    "trendy[\"tops\"] = (trendy[\"position\"]<=10)+0\n",
    "trendy.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the syntax is consistent. We use [ ]'s and a string to access the data in a column, it seems to only be fair that we can create a column in the same way.\n",
    "\n",
    "For persistence, we need to create another column, this one that will creates runs of whether a topic is in the top 10 or not. For that we compare the value of each entry to the entry from the time period just before it. The command shift() will take a column in a DataFrame and, well, shift it by one. So a column of [a,b,c] becomes [NaN,a,b], where the first entry in the shifted column is a missing value. \n",
    "\n",
    "In the code below we compare the shifted and unshifted columns, creating a True if the topic's status in one period was either NOT the same as that for the period immediately preceding it, or the two adjacent periods are separated by more than one samplingepoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trendy['block'] = ((trendy['tops']!= trendy['tops'].shift(1)) | (trendy['epoch'] - trendy['epoch'].shift(1) > 1 ))\n",
    "trendy.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trendy['block'] = ((trendy['tops']!= trendy['tops'].shift(1)) | (trendy['epoch'] - trendy['epoch'].shift(1) > 1 )).cumsum()\n",
    "trendy.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trendy[trendy[\"tops\"]==1].groupby(\"block\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we have a group of 10 sampling windows or about 2.5 hours. Here's the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trendy_tops = trendy[trendy[\"tops\"]==1]\n",
    "\n",
    "mydata = [go.Scatter(x=trendy_tops[\"datetime\"],y=trendy_tops[\"topic_name\"],mode=\"markers\",marker=go.Marker(size=10,color=trendy[\"block\"],colorscale='Viridis'))]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=400,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably, this is not really what we mean by persistence. The gaps out of the top 10 are really short. We might want to relax the definition a little and look at top 20 with maybe 2 or 3 epoch skips. That would let the topic drop off for 15 or 30 minutes but still come back and be part of the persistence window.\n",
    "\n",
    "**Your turn**\n",
    "\n",
    "Below is the essential portions of the code to calculate persistence. In cell 1 you set your targets. In 2 you define what it means to persist. You could look at top 10 or top 20 trends. You can say that one sampling window gap declares a new block or you could be more forgiving.\n",
    "\n",
    "Finally, we use the groupby() operation to determine the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. define target trend and city and subset the trends data\n",
    "target_trend = '#USofScience'\n",
    "city = 'Seattle'\n",
    "\n",
    "trendy = trends[(trends['topic_name'] == target_trend) & (trends['city']==city)].sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. add new columns to help with the grouping\n",
    "trendy[\"tops\"] = (trendy[\"position\"]<=10)+0\n",
    "trendy['block'] = ((trendy['tops']!= trendy['tops'].shift(1)) | (trendy['epoch'] - trendy['epoch'].shift(1) > 1 )).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Examine persistence\n",
    "trendy[trendy[\"tops\"]==1].groupby(\"block\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Make a plot\n",
    "trendy_tops = trendy[trendy[\"tops\"]==1]\n",
    "\n",
    "mydata = [go.Scatter(x=trendy_tops[\"datetime\"],y=trendy_tops[\"topic_name\"],mode=\"markers\",marker=go.Marker(size=10,color=trendy[\"block\"],colorscale='Viridis'))]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=400,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrence of a trend**\n",
    "\n",
    "The recurrence of a trend is the number of times the trend reappears in the TTL (Trending Topic List) after initially dropping out the TTL. \n",
    "\n",
    "The phenomena causing recurrence is intuitively more challenging to comprehend than persistence. Firstly, it makes sense to assume that if a trend can persist for longer its chances of recurrence are lower, because **sustained attention is hard!** Recurrence indicates disrupted or unsteady attention spans among users in the community. The repetition of the trend reappearing could be due to many factors, including reduction of attention of one trend caused due to a sudden relative increase in attention of the another trend.  \n",
    "\n",
    "Here's another fascinating tidbit about recurrence: **data shows that the origin location of a trend plays an important role in the recurrence score.** In fact, the recurrence score is higher if the location's population is larger and more diverse. For example, trends will recur more often in New York than Tallahassee. This is because a big city with diverse population tweeting many different things disperses attention more quickly compared to a more homogenous crowd of smaller cities where people might have limited topics to tweet about. \n",
    "\n",
    "Recurrence is also common after people wake up from sleep. Because you don't tweet in bed (or do you?)\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*4YreqD2g2mgtnrBlv0RNsw.gif\">\n",
    "\n",
    "In the previous code blocks, we saw 3 periods of time when the topic was active, meaning an initial window of popuarity and then 2 more. So it's recurrence is 2. Here we include the same basic code as above but the example is a different topic in a different city. It also computes the recurrence explicitly, using the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. define target trend and city and subset the trends data\n",
    "target_trend = 'Richard Spencer'\n",
    "city = 'Las Vegas'\n",
    "\n",
    "trendy = trends[(trends['topic_name'] == target_trend) & (trends['city']==city)].sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. add new columns to help with the grouping - here top 20 targets with a gap as big as 2 sampling times\n",
    "trendy[\"tops\"] = (trendy[\"position\"]<=20)+0\n",
    "trendy['block'] = ((trendy['tops']!= trendy['tops'].shift(1)) | (trendy['epoch'] - trendy['epoch'].shift(1) > 2 )).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Examine recurrence\n",
    "grps = trendy[trendy[\"tops\"]==1].groupby(\"block\")\n",
    "\n",
    "print \"The topic\",target_topic,\"recurs\", len(grps)-1, \"times in\", city\n",
    "print grps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Make a plot\n",
    "trendy_tops = trendy[trendy[\"tops\"]==1]\n",
    "\n",
    "mydata = [go.Scatter(x=trendy_tops[\"datetime\"],y=trendy_tops[\"topic_name\"],mode=\"markers\",marker=go.Marker(size=10,color=trendy[\"block\"],colorscale='Viridis'))]\n",
    "mylayout = go.Layout(autosize=False, width=1000,height=400,margin=go.Margin(l=150,r=50,b=100,t=100,pad=4))\n",
    "myfigure = go.Figure(data = mydata, layout = mylayout)\n",
    "iplot(myfigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drift**\n",
    "\n",
    "In simple terms, the drift of a trend is the chronological order of geo-locations that it touches on its way to becoming a national trend (sometimes it doesn't go national but only local). The reason we calculate drift is to observe two powerful network effects:\n",
    "\n",
    "* Drift can tell us which cities have low attention grasping capability, i.e. they can quickly catch up to another city's trending topic.\n",
    "* Drift can tell us which cities have similar interests, which is one of the reasons the trend spreads to that city.\n",
    "\n",
    "Shown below is the drift of #JesuisCharlie trend. It begins in Paris and then spreads to the French cities. However after that, it simultaneously drift to both some US cities (like NY, San Diego) and European cities (Madrid, Dusseldoff) within very short time. The final cities to get affected by the trend are South American and Australian cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*nmDuxI2vBA-R5gwIb1xjeg.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "\n",
    "Now let’s think about the bias issue. Bias means certain responses are more probable than others. This might cause a data sensor to detect some changes more promptly than others. Bias is not always social, it can be dependent on sampling. \n",
    "\n",
    "Sometimes, it is caused by the inherent signal generation. A nice example of this is determining which news articles are most read by users. One could pick a signal like ‘# of RTs the tweets with that news article received in Twitter’. But note Twitter has lots of bots, algorithm’s that could tweet out links based on domains or keywords. Thus, a link that has been RT-ed a lot might be under bots bias. On the other hand, think about an app like Instapaper, which flags a ‘read’ every time the user scrolls down the page to reach 20% distance from the end. This signal has much less bias, because bots cannot scroll. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithmic Curation\n",
    "\n",
    "* How do we start thinking about ways to have editors work in tandem with algorithms to identify trends. \n",
    "\n",
    "* What could happen if humans are not in the loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some more interesting reads about humans and algorithmic trend capture: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Article | Description |\n",
    "| ------ | ----------- |\n",
    "|1. [Fake news in Trends](https://www.washingtonpost.com/news/the-intersect/wp/2016/10/12/facebook-has-repeatedly-trended-fake-news-since-firing-its-human-editors/?utm_term=.ec1c1e47ca49)   |  Facebook fires editors, algorithm can't detect fake news. |\n",
    "|2. [Is this how the Trending Algorithm works?](https://qz.com/769413/heres-how-facebooks-automated-trending-bar-probably-works/) | And does that make it vulnerable? |\n",
    "|3. [The real problem with facebook and trending](https://stratechery.com/2016/the-real-problem-with-facebook-and-the-news/) | Is there a solution: editorial or algorithmic? |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweets in the wild**\n",
    "\n",
    "Twitter makes its data available via an Application Programming Interface or API. Think of it as a kind of web server that, instead of publishing HTML pages, offers you data. Web services are the soul of what we called Web 2.0. Data are interchangeable between machines allowing for automated processing. In this case, we are looking at trends, but the underlying Tweet data is available too. \n",
    "\n",
    "For a series of services to share data, you obviously have to agree on what the data looks like. It's format. Twitter has a [published format for its tweets.](https://dev.twitter.com/overview/api/tweets) Have a look! Tweets are stored in JSON objects, where JSON stands for the Javascript Object Notation. It looks a lot like native Python objects. We'll explain a bit more shortly.\n",
    "\n",
    "We have captured 20,000 raw tweets from the DC area at noon during the inauguration. The data set is located here. Download it and move it to the folder where you have placed this notebook. Let's look at a tweet! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "tweet = loads(open(\"inauguration_data/noon_tweets/822503956907773952.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand a tweet, we need one more built-in object in Python -- a dictionary. While lists let you store data sequentially (a fist object, a second and a last), dictionaries store data using words (technically so-called immutable objects). Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mini_tweet = {'created_at': 'Fri Jan 20 17:59:59 +0000 2017',\n",
    "              'source': '<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>',\n",
    "              'text': 'millwoodschool Our upper school boys are bonding on the slopes! #Wintergreen #AnnualSkiTrip @\\u2026 https://t.co/XYn5wDVJAt',\n",
    "              'user': {'followers_count': 121,\n",
    "                       'friends_count': 121,\n",
    "                       'id': 608401769,\n",
    "                       'lang': 'en',\n",
    "                       'location': 'Richmond, VA',\n",
    "                       'name': 'ChristensCreations',\n",
    "                       'statuses_count': 2778}\n",
    "              }\n",
    "\n",
    "print mini_tweet[\"created_at\"],\"\\n\"\n",
    "print mini_tweet[\"text\"],\"\\n\"\n",
    "print mini_tweet[\"user\"][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary, like a list, can hold anything. Lists, other dictionaries, boolean, numeric data... you name it. We are going to work with these tweets to pull out trends. What kind of data might be interesting? Have a look at some tweets and see what you can find.\n",
    "\n",
    "**A first pass**\n",
    "\n",
    "We have taken tweets from the noon period and boiled them down into a CSV for you. Have a look. Let's talk about what you need to make your own trending algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_option(\"display.max_colwidth\",140)\n",
    "\n",
    "tweets = read_csv(\"inauguration_data/inauguration_tweets_at_noon.csv\")\n",
    "tweets.head(50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
